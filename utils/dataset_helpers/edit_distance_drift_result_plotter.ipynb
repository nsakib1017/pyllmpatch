{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91fe41e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef65ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pypi_decompiled_clustered = pd.read_csv(f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_syntax_errors_clustered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "769fd799",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OF_DECOMPILATION_RESULTS = {\n",
    "    \"gemini-flash\": {\"base_result\": f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_gemini_flash.csv\"},\n",
    "    \"qwen-7b\": {\n",
    "        \"base_result\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_qwen-7b_config_0.csv\",\n",
    "        \"config_1_results\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_qwen-7b_config_1.csv\",\n",
    "        \"config_2_results\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_qwen-7b_config_2.csv\",\n",
    "        },\n",
    "    \"qwen-32b\": {\n",
    "        \"base_result\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_qwen-32b_config_0.csv\",\n",
    "        \"config_1_results\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_qwen-32b_config_1.csv\",\n",
    "        \"config_2_results\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_qwen-32b_config_2.csv\",\n",
    "        },\n",
    "    \"deepseek-r1\": {\n",
    "        \"base_result\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_deepseek-r1_config_0.csv\",\n",
    "        \"config_1_results\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_deepseek-r1_config_1.csv\",\n",
    "        \"config_2_results\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_deepseek-r1_config_2.csv\",\n",
    "        },\n",
    "    \"granite\": {\n",
    "        \"base_result\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_granite_config_0.csv\",\n",
    "        \"config_1_results\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_granite_config_1.csv\",\n",
    "        \"config_2_results\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_granite_config_2.csv\",\n",
    "        },\n",
    "    \"mistral\": {\n",
    "        \"base_result\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_mistral_config_0.csv\",\n",
    "        \"config_1_results\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_mistral_config_1.csv\",\n",
    "        \"config_2_results\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_mistral_config_2.csv\",\n",
    "        },\n",
    "    \"phi-4\": {\n",
    "        \"base_result\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_phi-4_config_0.csv\",\n",
    "        \"config_1_results\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_phi-4_config_1.csv\",\n",
    "        \"config_2_results\":f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_comparison_results_phi-4_config_2.csv\",\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec711d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_CSV_PATH = f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decompiled_syntax_errors_clustered.csv\"\n",
    "CLUSTER_COL = \"cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03329f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_MERGED_CSV = f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/decomp_results_with_clusters_and_metrics.csv\"\n",
    "OUT_CLUSTER_STATS_CSV = f\"{os.getenv('PROJECT_ROOT_DIR')}/dataset/cluster_summary_stats.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5868be32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_config(run_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts keys like:\n",
    "      base_result -> config_0\n",
    "      config_1_results -> config_1\n",
    "    \"\"\"\n",
    "    if run_key == \"base_result\":\n",
    "        return \"config_0\"\n",
    "    m = re.search(r\"config_(\\d+)\", run_key)\n",
    "    return f\"config_{m.group(1)}\" if m else run_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65f0b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_results(path_map: dict, strict: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads every CSV from PATH_OF_DECOMPILATION_RESULTS and adds:\n",
    "      model, config, run_key, source_csv\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for model, runs in path_map.items():\n",
    "        for run_key, csv_path in runs.items():\n",
    "            config = infer_config(run_key)\n",
    "\n",
    "            if not os.path.exists(csv_path):\n",
    "                msg = f\"Missing file: {csv_path}\"\n",
    "                if strict:\n",
    "                    raise FileNotFoundError(msg)\n",
    "                print(f\"[skip] {msg}\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(csv_path)\n",
    "            df[\"model\"] = model\n",
    "            df[\"config\"] = config\n",
    "            df[\"run_key\"] = run_key\n",
    "            df[\"source_csv\"] = csv_path\n",
    "            dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83b95fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_results(path_map: dict, strict: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads every CSV from PATH_OF_DECOMPILATION_RESULTS and adds:\n",
    "      model, config, run_key, source_csv\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for model, runs in path_map.items():\n",
    "        for run_key, csv_path in runs.items():\n",
    "            config = infer_config(run_key)\n",
    "\n",
    "            if not os.path.exists(csv_path):\n",
    "                msg = f\"Missing file: {csv_path}\"\n",
    "                if strict:\n",
    "                    raise FileNotFoundError(msg)\n",
    "                print(f\"[skip] {msg}\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(csv_path)\n",
    "            df[\"model\"] = model\n",
    "            df[\"config\"] = config\n",
    "            df[\"run_key\"] = run_key\n",
    "            df[\"source_csv\"] = csv_path\n",
    "            dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3488aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percent_changes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes:\n",
    "      syntactic_drift_pct\n",
    "      cosine_distance_pct_change\n",
    "\n",
    "    Assumes df has:\n",
    "      d_lookup_vs_decompiled, d_lookup_vs_repaired\n",
    "      d_lookup_vs_decompiled_cosine_distance, d_lookup_vs_repaired_cosine_distance\n",
    "    \"\"\"\n",
    "    required = [\n",
    "        \"d_lookup_vs_decompiled\",\n",
    "        \"d_lookup_vs_repaired\",\n",
    "        \"d_lookup_vs_decompiled_cosine_distance\",\n",
    "        \"d_lookup_vs_repaired_cosine_distance\",\n",
    "    ]\n",
    "\n",
    "    # Ensure numeric\n",
    "    for c in required:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows with NaNs in required inputs\n",
    "    df = df.dropna(subset=required).copy()\n",
    "\n",
    "    # Avoid divide-by-zero\n",
    "    denom_d = df[\"d_lookup_vs_decompiled\"].replace(0, np.nan)\n",
    "    denom_dist = df[\"d_lookup_vs_decompiled_cosine_distance\"].replace(0, np.nan)\n",
    "\n",
    "    df[\"syntactic_drift_pct\"] = (\n",
    "        (df[\"d_lookup_vs_repaired\"] - df[\"d_lookup_vs_decompiled\"]) / denom_d\n",
    "    ) * 100\n",
    "\n",
    "    df[\"cosine_distance_pct_change\"] = (\n",
    "        (df[\"d_lookup_vs_repaired_cosine_distance\"] - df[\"d_lookup_vs_decompiled_cosine_distance\"]) / denom_dist\n",
    "    ) * 100\n",
    "    df[[\"syntactic_drift_pct\", \"cosine_distance_pct_change\"]] = (\n",
    "        df[[\"syntactic_drift_pct\", \"cosine_distance_pct_change\"]].round(2)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89421892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_by_cluster(df: pd.DataFrame, cluster_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Group stats by model/config/cluster.\n",
    "    \"\"\"\n",
    "    grp_cols = [\"model\", \"config\", cluster_col]\n",
    "    stats = (\n",
    "        df.groupby(grp_cols, as_index=False)\n",
    "          .agg(\n",
    "              n=(\"file_hash\", \"count\"),\n",
    "              drift_mean=(\"syntactic_drift_pct\", \"mean\"),\n",
    "              drift_median=(\"syntactic_drift_pct\", \"median\"),\n",
    "              drift_std=(\"syntactic_drift_pct\", \"std\"),\n",
    "              dist_mean=(\"cosine_distance_pct_change\", \"mean\"),\n",
    "              dist_median=(\"cosine_distance_pct_change\", \"median\"),\n",
    "              dist_std=(\"cosine_distance_pct_change\", \"std\"),\n",
    "          )\n",
    "          .sort_values([\"model\", \"config\", \"n\"], ascending=[True, True, False])\n",
    "    )\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa88324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] merged+metrics saved: /home/diogenes/pylingual_colaboration/pylingual_download/code/dataset/decomp_results_with_clusters_and_metrics.csv\n",
      "[ok] cluster summary saved: /home/diogenes/pylingual_colaboration/pylingual_download/code/dataset/cluster_summary_stats.csv\n",
      "           model    config    cluster  drift_mean  drift_median  drift_std  \\\n",
      "2    deepseek-r1  config_0  cluster_4       38.34          0.00     126.23   \n",
      "0    deepseek-r1  config_0  cluster_1        5.48          0.00      34.60   \n",
      "1    deepseek-r1  config_0  cluster_3       15.95         15.95      27.59   \n",
      "3    deepseek-r1  config_1  cluster_4      250.97        250.97        NaN   \n",
      "5   gemini-flash  config_0  cluster_1       77.29          0.00     666.72   \n",
      "..           ...       ...        ...         ...           ...        ...   \n",
      "67       qwen-7b  config_2  cluster_4       32.56          0.70      72.01   \n",
      "66       qwen-7b  config_2  cluster_3      -39.59        -24.22      44.65   \n",
      "68       qwen-7b  config_2  cluster_5       20.89          3.78      31.36   \n",
      "65       qwen-7b  config_2  cluster_2       16.38         16.38      23.16   \n",
      "63       qwen-7b  config_2  cluster_0        0.00          0.00        NaN   \n",
      "\n",
      "    dist_mean  dist_median  dist_std  \n",
      "2        0.93        -1.79     22.88  \n",
      "0        5.86         7.10     36.98  \n",
      "1       36.40        36.40     43.74  \n",
      "3       23.09        23.09       NaN  \n",
      "5        7.41        -4.41    187.49  \n",
      "..        ...          ...       ...  \n",
      "67       0.52         0.02     20.83  \n",
      "66     -23.73        -8.68     49.00  \n",
      "68     -17.10        -4.27     23.47  \n",
      "65     -16.39       -16.39     41.66  \n",
      "63      -1.93        -1.93       NaN  \n",
      "\n",
      "[69 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load results from all models/configs\n",
    "df_results = load_all_results(PATH_OF_DECOMPILATION_RESULTS, strict=False)\n",
    "if df_results.empty:\n",
    "    raise RuntimeError(\"No results CSVs were loaded. Check your paths.\")\n",
    "\n",
    "# Load clusters\n",
    "if not os.path.exists(CLUSTER_CSV_PATH):\n",
    "    raise FileNotFoundError(f\"Cluster CSV not found: {CLUSTER_CSV_PATH}\")\n",
    "\n",
    "df_clusters = pd.read_csv(CLUSTER_CSV_PATH)\n",
    "\n",
    "needed_cluster_cols = {\"file_hash\", CLUSTER_COL}\n",
    "missing = needed_cluster_cols - set(df_clusters.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Cluster CSV is missing columns: {sorted(missing)}\")\n",
    "\n",
    "# Keep one cluster label per file_hash (if duplicates exist)\n",
    "df_clusters = df_clusters[[\"file_hash\", CLUSTER_COL]].drop_duplicates(\"file_hash\")\n",
    "\n",
    "# Merge cluster info into results\n",
    "df_merged = df_results.merge(\n",
    "        df_clusters,\n",
    "        on=\"file_hash\",\n",
    "        how=\"left\",\n",
    "        validate=\"many_to_one\",  # many results rows per file_hash is OK\n",
    "    )\n",
    "\n",
    "    # If you want to drop rows with no cluster label, uncomment:\n",
    "    # df_merged = df_merged.dropna(subset=[CLUSTER_COL]).copy()\n",
    "\n",
    "    # Your cleanup line (expanded to include cosine distance columns)\n",
    "df_merged.dropna(\n",
    "        subset=[\n",
    "            \"d_lookup_vs_decompiled\",\n",
    "            \"d_lookup_vs_repaired\",\n",
    "            \"d_lookup_vs_decompiled_cosine_distance\",\n",
    "            \"d_lookup_vs_repaired_cosine_distance\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Compute percent-change metrics\n",
    "df_merged = compute_percent_changes(df_merged)\n",
    "\n",
    "# Cluster-based summary\n",
    "cluster_stats = summarize_by_cluster(df_merged, CLUSTER_COL)\n",
    "cluster_stats = cluster_stats.round(2)\n",
    "    # Save outputs\n",
    "cluster_stats = cluster_stats.drop(columns=[\"n\"], errors=\"ignore\")\n",
    "df_merged.to_csv(OUT_MERGED_CSV, index=False)\n",
    "cluster_stats.to_csv(OUT_CLUSTER_STATS_CSV, index=False)\n",
    "\n",
    "print(f\"[ok] merged+metrics saved: {OUT_MERGED_CSV}\")\n",
    "print(f\"[ok] cluster summary saved: {OUT_CLUSTER_STATS_CSV}\")\n",
    "print(cluster_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyllmpatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
